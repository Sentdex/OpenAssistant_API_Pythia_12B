{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Assistant as a Local ChatGPT API\n",
    "\n",
    "Video tutorial:\n",
    "[![Open Assistant as a Local ChatGPT API](https://img.youtube.com/vi/kkTNg_UOCNE/0.jpg)](https://www.youtube.com/watch?v=kkTNg_UOCNE)\n",
    "\n",
    "\n",
    "Welcome everyone to a bit of a showcasing and how-to with Open Assistant's Pythia 12 billion parameter model. This model is meant to be a chat assistant, like ChatGPT, but runnable locally. The model uses 48GB of memory, or 24GB at half precision.\n",
    "\n",
    "This model is in live development and training, so you will want to keep an eye out for new releases. I started playing with this model's first variant (https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) and the next time I checked for an update, there was a 4th iteration available (https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5). \n",
    "\n",
    "\n",
    "Being a local model, I'd like to also show how to essentially set up your own local API, which makes doing your own R&D and testing much quicker and easier. To start though, let's check out a super basic example. \n",
    "\n",
    "At their most basic level, these large language GPT models just simply generate text sequentially. An example input might be:\n",
    "\n",
    "\"<|prompter|>What is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\n",
    "\n",
    "And the output might be.\n",
    "\n",
    "\"A meme is a cultural idea, behavior, or style that spreads from person to person within a\"\n",
    "\n",
    "We can then wrap this in some basic logic to handle for the special tokens of <|prompter|>, <|endoftext|>, and <|assistant|> to get a more human readable output to give the chat and response feel. \n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL TO RUN ON A SPECIFIC GPU:\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import AutoTokenizer & AutoModelForCausalLM, which will allow us to load the model and tokenizer from the HuggingFace model hub. We'll also import torch, which we'll use to handle the model's output in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will load the model and tokenizer into memory. If this is the first time you're running this with that specific model, it will take a bit to download the model and tokenizer. After that, it should take ~ a minute or so to load into memory. Once you have the model downloaded and loaded, you can optionally move it to your GPU if possible. In this example, I am also using the half precision version of the model, which is a bit faster and uses half the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to GPU and set it to half precision (float16)\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll start with some input. This could be any text you want, but it probably makes the most sense to structure it how this model was trained, with the special tokens of <|prompter|>, <|endoftext|>, and <|assistant|>. \n",
    "\n",
    "Imagine that we want to ask this model \"What color is the sky?\"\n",
    "\n",
    "The way to build this prompt would be to be more like:\n",
    "\n",
    "\"<|prompter|>What color is the sky?<|endoftext|><|assistant>\"\n",
    "\n",
    "It feels a bit weird to use this end of ext tag followed by assistant tag, seems maybe redundant, but that's in the example provided by OpenAssistant on their \n",
    "HF page, so I assume every string before another \"speaker\" is terminated with that tag. By ending with the <|assistant> tag, we're making very clear to the model that a continue generation would be starting with the assistance's response to that input. The output from the model will likely be a continued generation, something like:\n",
    "\n",
    "\"<|prompter|>What color is the sky?<|endoftext|><|assistant> The sky is often blue.<|endoftext|>\"\n",
    "\n",
    "You may find that after that end text tag, another prompter tag is generated and more text is continued to be generated by the model. You can either handle for this with some python logic to stop at the end of text tag, or you can utilize the early-stopping capability from the transformers package.\n",
    "\n",
    "Let's see how to do this in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"What color is the sky?\"\n",
    "\n",
    "input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input to GPU  (ONLY do this if you're using the GPU for your model.)\n",
    "input_ids = input_ids.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify some text input, then we tokenize that input with the model's tokenizer. From here, we move the tokenized input to the GPU, if we're using one. \n",
    "\n",
    "Next, we're going torch's automatic mixed precision (AMP) autocast context manager, which automatically sets operation datatypes. Within AMP's autocast context, we'll generate output with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using automatic mixed precision\n",
    "with torch.cuda.amp.autocast():\n",
    "    # generate text until the output length (which includes the original input/context's length) reaches max_length. do_sample for random sampling vs greedy\n",
    "    output = model.generate(input_ids, max_length=2048, do_sample=True, early_stopping=True, num_return_sequences=1, eos_token_id=model.config.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got some output, but its on the GPU. Let's move it to the CPU so we can more easily access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the output back to CPU\n",
    "output = output.cpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the tokenizer to decode the output into human readable text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# OPTIONAL TO RUN ON A SPECIFIC GPU:\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move the model to GPU and set it to half precision (float16)\n",
    "model = model.half().cuda()\n",
    "\n",
    "inp = \"What color is the sky?\"\n",
    "\n",
    "input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "\n",
    "# Move the input to GPU  (ONLY do this if you're using the GPU for your model.)\n",
    "input_ids = input_ids.cuda()\n",
    "\n",
    "# Using automatic mixed precision\n",
    "with torch.cuda.amp.autocast():\n",
    "    # generate text until the output length (which includes the original input/context's length) reaches max_length\n",
    "    output = model.generate(input_ids, max_length=2048, do_sample=True, early_stopping=True, num_return_sequences=1, eos_token_id=model.config.eos_token_id)\n",
    "\n",
    "# Move the output back to CPU\n",
    "output = output.cpu()\n",
    "# Decode the output\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "print(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's a very basic example of how to use this model. Let's take a look at how to set up a local API to make this a bit easier to use and workwith. \n",
    "\n",
    "With an API, even just locally, we can speed up R&D time without needing to re-load the model to memory every run (though you could also just use a notebook or something in this case too!). Beyond that, we can also access this API from anywhere else on our network, or even the internet if we wanted, empowering whatever devices and computers we might want.\n",
    "\n",
    "For this, I am going to use Flask (pip install flask), but there are certainly many ways you could do this same thing. I'll start a new script, which I'll call `oasst_api.py`. We'll start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "MODEL_NAME = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = model.half().cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too much new here from before yet, other than the flask imports and beginning app defintion. Now, all we need with our flask app is a very basic route to handle our input and output. We'll use the same logic as before, but we'll wrap it in a function, and then we'll use flask's jsonify to return the output as a json object.\n",
    "\n",
    "Starting with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    content = request.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This view will take a post request, and that request will have a json object, which will contain our prompt. We can get the prompt with `content.get` and then we will tokenize and pass that to the GPU (if we're using one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    inp = content.get(\"text\", \"\")\n",
    "    input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will query the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with torch.cuda.amp.autocast():\n",
    "        output = model.generate(input_ids, max_length=2048, do_sample=True, early_stopping=True, num_return_sequences=1, eos_token_id=model.config.eos_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we're using AMP's autocast and model.generate to get our output. From here, we just need to decode and return the output as a json object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    return jsonify({'generated_text': decoded})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making our full code for `oasst_api.py` now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "MODEL_NAME = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = model.half().cuda()\n",
    "\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    content = request.json\n",
    "    inp = content.get(\"text\", \"\")\n",
    "    input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = model.generate(input_ids, max_length=2048, do_sample=True, early_stopping=True, num_return_sequences=1, eos_token_id=model.config.eos_token_id)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "    return jsonify({'generated_text': decoded})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)  # Set the host to '0.0.0.0' to make it accessible from your local network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run this API on whatever machine we want to host the model, and then we can query this machine from whatever machine we want, provided it's on network. \n",
    "\n",
    "For example, I can create a new file, called `chat-oasst-api.py` to work with my new API. To start, some imports and constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import colorama\n",
    "\n",
    "SERVER_IP = \"10.0.0.18\" # Change this to the IP of your server that's hosting the API. This can be the same machine you're working on too.\n",
    "URL = f\"http://{SERVER_IP}:5000/generate\"\n",
    "\n",
    "USERTOKEN = \"<|prompter|>\"\n",
    "ENDTOKEN = \"<|endoftext|>\"\n",
    "ASSISTANTTOKEN = \"<|assistant|>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the imports and constants out of the way, let's write a quick prompt function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(inp):\n",
    "    data = {\"text\": inp}\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "\n",
    "    response = requests.post(URL, data=json.dumps(data), headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"generated_text\"]\n",
    "    else:\n",
    "        return \"Error:\", response.status_code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes input, builds a dictionary which we'll convert to a json object, sets headers, and then sends a post request to our API. We'll use the requests package to do this. From here, we'll grab either the json response, or error if there is one. Now we just need some simple logic to handle for the chat and context:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = \"\"\n",
    "while True:\n",
    "    inp = input(\">>> \")\n",
    "    context = history + USERTOKEN + inp + ENDTOKEN + ASSISTANTTOKEN\n",
    "    output = prompt(context)\n",
    "    history = output\n",
    "    just_latest_asst_output = output.split(ASSISTANTTOKEN)[-1].split(ENDTOKEN)[0]\n",
    "    # color just_latest_asst_output green in print:\n",
    "    print(colorama.Fore.GREEN + just_latest_asst_output + colorama.Style.RESET_ALL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full `chat_oasst_api.py` code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import colorama\n",
    "\n",
    "SERVER_IP = \"10.0.0.18\"\n",
    "URL = f\"http://{SERVER_IP}:5000/generate\"\n",
    "\n",
    "USERTOKEN = \"<|prompter|>\"\n",
    "ENDTOKEN = \"<|endoftext|>\"\n",
    "ASSISTANTTOKEN = \"<|assistant|>\"\n",
    "\n",
    "def prompt(inp):\n",
    "    data = {\"text\": inp}\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "\n",
    "    response = requests.post(URL, data=json.dumps(data), headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"generated_text\"]\n",
    "    else:\n",
    "        return \"Error:\", response.status_code\n",
    "    \n",
    "history = \"\"\n",
    "while True:\n",
    "    inp = input(\">>> \")\n",
    "    context = history + USERTOKEN + inp + ENDTOKEN + ASSISTANTTOKEN\n",
    "    output = prompt(context)\n",
    "    history = output\n",
    "    just_latest_asst_output = output.split(ASSISTANTTOKEN)[-1].split(ENDTOKEN)[0]\n",
    "    # color just_latest_asst_output green in print:\n",
    "    print(colorama.Fore.GREEN + just_latest_asst_output + colorama.Style.RESET_ALL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can fully interact with our model! \n",
    "\n",
    "Only one slight problem is the context is going to continue growing. The maximum context length for this model is 2048 tokens. That's quite a bit, but if you have a longer conversation, or you even just want to keep an ongoing one for days, this is going to be a problem. \n",
    "\n",
    "How you handle for context might vary. You could just trim context to keep it in some range. Remember: context includes the prompt as well as the generation. Your generation might want to be 200 tokens long, so this really means your prompt needs to be 1848 tokens or less.\n",
    "\n",
    "Besides a simple trimming past a certain amount of tokens, you could also get more complex by attempting to also summarize the context to \"compress\" it. I will skip that for now and go straight to a trim. In most cases, this will be fine. If you need to retain history more, then you might try a more complicated approach. \n",
    "\n",
    "You can also choose whether you want to add this logic to the API, or the client. I think handling for summarization would be done client-side, but a brute trimming of the context to handle for longer conversations can happen API-side I think. This really is up to you though. I'll edit the `oasst_api.py`, and start by adding the following constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max context length and the determine cushion for response\n",
    "MAX_CONTEXT_LENGTH = model.config.max_position_embeddings\n",
    "print(f\"Max context length: {MAX_CONTEXT_LENGTH}\")\n",
    "ROOM_FOR_RESPONSE = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dynamically pulls the maximum context length from the model's attributes, and then we can opt for how much of a \"cushion\" we want to leave for a plausible generation. I've chosen 512, which is quite large and probably will never happen, but 2048-512=1536, which is still a lot of context!\n",
    "\n",
    "Now, within the `generate` function, we can add some logic to handle for context length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Calc current size\n",
    "    print(\"Context length is currently\", input_ids.shape[1], \"tokens. Allowed amount is\", MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE, \"tokens.\")\n",
    "    # determine if we need to trim\n",
    "    if input_ids.shape[1] > (MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE):\n",
    "        print(\"Trimming a bit\")\n",
    "        # trim as needed AT the first dimension\n",
    "        input_ids = input_ids[:, -(MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code for `oasst_api.py` is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "MODEL_NAME = \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Get max context length and the determine cushion for response\n",
    "MAX_CONTEXT_LENGTH = model.config.max_position_embeddings\n",
    "print(f\"Max context length: {MAX_CONTEXT_LENGTH}\")\n",
    "ROOM_FOR_RESPONSE = 512\n",
    "\n",
    "model = model.half().cuda()\n",
    "\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    content = request.json\n",
    "    inp = content.get(\"text\", \"\")\n",
    "    input_ids = tokenizer.encode(inp, return_tensors=\"pt\")\n",
    "\n",
    "    # Calc current size\n",
    "    print(\"Context length is currently\", input_ids.shape[1], \"tokens. Allowed amount is\", MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE, \"tokens.\")\n",
    "    # determine if we need to trim\n",
    "    if input_ids.shape[1] > (MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE):\n",
    "        print(\"Trimming a bit\")\n",
    "        # trim as needed AT the first dimension\n",
    "        input_ids = input_ids[:, -(MAX_CONTEXT_LENGTH-ROOM_FOR_RESPONSE):]\n",
    "    \n",
    "    input_ids = input_ids.cuda()\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = model.generate(input_ids, max_length=2048, do_sample=True, early_stopping=True, num_return_sequences=1, eos_token_id=model.config.eos_token_id)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "    return jsonify({'generated_text': decoded})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)  # Set the host to '0.0.0.0' to make it accessible from your local network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
